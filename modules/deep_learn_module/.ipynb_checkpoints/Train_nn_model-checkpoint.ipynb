{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import nn_models\n",
    "\n",
    "import sys, os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..')) \n",
    "import custom_utils\n",
    "\n",
    "\n",
    "def init_dirs(config_file):\n",
    "    out_dir = custom_utils.create_out_dir(config_file)\n",
    "    out_dir = '../' + out_dir\t\n",
    "    ml_data_dir = out_dir + '/ml_data'\n",
    "\n",
    "    return out_dir, ml_data_dir\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    print('> Loading data (top ' + str(100 * float(top_ratio)) + ' %) ...')\n",
    "    top_ratio_str = '.top_' + str(top_ratio)\n",
    "\n",
    "    print('Reading training data...')\n",
    "    pkl_in = open(ml_data_dir + '/train' + top_ratio_str + '.pkl', 'rb')\n",
    "    train_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    print('Reading validation data...')\n",
    "    pkl_in = open(ml_data_dir + '/validation' + top_ratio_str + '.pkl', 'rb')\n",
    "    validation_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    print('Reading test data...')\n",
    "    pkl_in = open(ml_data_dir + '/test' + top_ratio_str + '.pkl', 'rb')\n",
    "    test_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    return train_dict, validation_dict, test_dict\n",
    "\n",
    "\n",
    "def inspect_input_data(train_dict, validation_dict, test_dict):\n",
    "\n",
    "    print('\\n> Train:')\n",
    "    print(train_dict['X'].shape)\n",
    "    print(train_dict[y].shape)\n",
    "    print(train_dict['seqs'].shape)\n",
    "\n",
    "    print('\\n> Validation:')\n",
    "    print(validation_dict['X'].shape)\n",
    "    print(validation_dict[y].shape)\n",
    "    print(validation_dict['seqs'].shape)\n",
    "\n",
    "    print('\\n> Test:')\n",
    "    print(test_dict['X'].shape)\n",
    "    print(test_dict[y].shape)\n",
    "    print(test_dict['seqs'].shape)\n",
    "\n",
    "\n",
    "\n",
    "def compile_and_train_model(model, epochs=40):\n",
    "\n",
    "    if regression:\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['cosine'])\n",
    "\n",
    "    checkpoint_name = 'gwrvis_best_model.hdf5'\n",
    "    checkpointer = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    model.fit(train_dict['seqs'], train_dict[y], batch_size=2048, epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_data=(validation_dict['seqs'], validation_dict[y]), \n",
    "          callbacks=[checkpointer,earlystopper])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_and_evaluate_model(model):\n",
    "    test_results = model.evaluate(test_dict['seqs'], test_dict[y]) \n",
    "    print(test_results)\n",
    "\n",
    "    preds = model.predict(test_dict['seqs'])\n",
    "    decision_thres = 0.5 # for classification\n",
    "    if regression:\n",
    "        decision_thres = 0 # 0 is the natural border between tolerant and intolerant gwRVIS values\n",
    "\n",
    "    preds[preds >= decision_thres] = 1\n",
    "    preds[preds < decision_thres] = 0\n",
    "\n",
    "    preds_flat = preds.flatten()\n",
    "    test_flat = test_dict[y].flatten()\n",
    "\n",
    "    print(accuracy_score(test_flat, preds_flat))\n",
    "    print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "    roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "    print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading data (top 0.1 %) ...\n",
      "Reading training data...\n",
      "Reading validation data...\n",
      "Reading test data...\n",
      "\n",
      "> Train:\n",
      "(1240, 5)\n",
      "(1240, 2)\n",
      "(1240, 3000, 4)\n",
      "\n",
      "> Validation:\n",
      "(64, 5)\n",
      "(64, 2)\n",
      "(64, 3000, 4)\n",
      "\n",
      "> Test:\n",
      "(310, 5)\n",
      "(310, 2)\n",
      "(310, 3000, 4)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 2971, 16)          1936      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 198, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 198, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3168)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               405632    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 407,826\n",
      "Trainable params: 407,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1240 samples, validate on 64 samples\n",
      "Epoch 1/40\n",
      "1240/1240 [==============================] - 4s 3ms/step - loss: 0.8506 - cosine_proximity: -0.6392 - val_loss: 4.7580 - val_cosine_proximity: -0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.75802, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 2/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 4.7275 - cosine_proximity: -0.4960 - val_loss: 1.9692 - val_cosine_proximity: -0.5105\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.75802 to 1.96921, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 3/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 2.0405 - cosine_proximity: -0.5062 - val_loss: 0.8336 - val_cosine_proximity: -0.6359\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.96921 to 0.83362, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 4/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7986 - cosine_proximity: -0.6547 - val_loss: 1.2464 - val_cosine_proximity: -0.5480\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.83362\n",
      "Epoch 5/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 1.2217 - cosine_proximity: -0.5548 - val_loss: 1.1625 - val_cosine_proximity: -0.5582\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.83362\n",
      "Epoch 6/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 1.1459 - cosine_proximity: -0.5638 - val_loss: 0.9398 - val_cosine_proximity: -0.6010\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83362\n",
      "Epoch 7/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.9340 - cosine_proximity: -0.6054 - val_loss: 0.7607 - val_cosine_proximity: -0.6673\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83362 to 0.76069, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 8/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7666 - cosine_proximity: -0.6657 - val_loss: 0.6935 - val_cosine_proximity: -0.7069\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.76069 to 0.69352, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 9/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.6951 - cosine_proximity: -0.7062 - val_loss: 0.6920 - val_cosine_proximity: -0.7079\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69352 to 0.69198, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 10/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.6975 - cosine_proximity: -0.7042 - val_loss: 0.7145 - val_cosine_proximity: -0.6930\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69198\n",
      "Epoch 11/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7136 - cosine_proximity: -0.6939 - val_loss: 0.7351 - val_cosine_proximity: -0.6807\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69198\n",
      "Epoch 12/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7318 - cosine_proximity: -0.6827 - val_loss: 0.7444 - val_cosine_proximity: -0.6754\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69198\n",
      "Epoch 13/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7418 - cosine_proximity: -0.6769 - val_loss: 0.7418 - val_cosine_proximity: -0.6768\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69198\n",
      "Epoch 14/40\n",
      "1240/1240 [==============================] - 3s 2ms/step - loss: 0.7393 - cosine_proximity: -0.6781 - val_loss: 0.7334 - val_cosine_proximity: -0.6815\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69198\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    config_file = '../config.yaml' #sys.argv[1]\n",
    "    top_ratio = 0.001 #sys.argv[2] \t#default: 0.01 -- look at top 1% of intolerant/tolerant windows\n",
    "\n",
    "    regression=False\n",
    "    if regression:\n",
    "        y = 'gwrvis'   # continuous value\n",
    "    else:\n",
    "        y = 'y'  # 1/0 annotation\n",
    "\n",
    "    config_params = custom_utils.get_config_params(config_file)\n",
    "    win_len = config_params['win_len']\n",
    "\n",
    "    # init out dir structure\n",
    "    out_dir, ml_data_dir = init_dirs(config_file)\n",
    "\n",
    "    # read train, validation, test data\n",
    "    train_dict, validation_dict, test_dict = read_data()\n",
    "\n",
    "    # print input data shapes to check for consistency\n",
    "    inspect_input_data(train_dict, validation_dict, test_dict)\n",
    "\n",
    "    model = nn_models.cnn_1_conv_2_fcc(regression=regression) \n",
    "#     model = nn_models.cnn_rnn_1_conv_1_lstm(regression=regression)\n",
    "    print(model.summary())\n",
    "\n",
    "    model = compile_and_train_model(model, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6666713  0.3333287 ]\n",
      " [0.6330297  0.36697033]\n",
      " [0.6516117  0.3483883 ]\n",
      " [0.6495804  0.3504196 ]\n",
      " [0.70192283 0.29807717]\n",
      " [0.6854742  0.31452578]\n",
      " [0.67706525 0.32293475]\n",
      " [0.69077575 0.3092243 ]\n",
      " [0.6583584  0.34164155]\n",
      " [0.64561146 0.35438856]\n",
      " [0.71210897 0.28789103]\n",
      " [0.6467439  0.35325608]\n",
      " [0.6849443  0.3150557 ]\n",
      " [0.6910312  0.3089688 ]\n",
      " [0.63427615 0.3657238 ]\n",
      " [0.6569383  0.34306175]\n",
      " [0.65711826 0.34288174]\n",
      " [0.69592595 0.30407408]\n",
      " [0.6324239  0.3675761 ]\n",
      " [0.66454005 0.33545995]\n",
      " [0.6633434  0.3366566 ]\n",
      " [0.6832921  0.3167079 ]\n",
      " [0.6547281  0.34527186]\n",
      " [0.67860126 0.32139876]\n",
      " [0.67624205 0.32375792]\n",
      " [0.67345697 0.32654306]\n",
      " [0.6442981  0.35570183]\n",
      " [0.68952787 0.3104721 ]\n",
      " [0.63235587 0.36764407]\n",
      " [0.63992333 0.36007664]\n",
      " [0.6738321  0.3261679 ]\n",
      " [0.6450755  0.35492456]\n",
      " [0.6224532  0.37754676]\n",
      " [0.6539317  0.3460683 ]\n",
      " [0.6467276  0.35327238]\n",
      " [0.6616685  0.33833155]\n",
      " [0.65128213 0.34871784]\n",
      " [0.66833425 0.33166578]\n",
      " [0.6456256  0.35437438]\n",
      " [0.6420474  0.3579526 ]\n",
      " [0.6370951  0.36290494]\n",
      " [0.67505145 0.32494852]\n",
      " [0.6685589  0.33144107]\n",
      " [0.61770046 0.3822995 ]\n",
      " [0.6252846  0.3747154 ]\n",
      " [0.6829901  0.3170099 ]\n",
      " [0.655553   0.344447  ]\n",
      " [0.6622068  0.33779317]\n",
      " [0.69295913 0.30704084]\n",
      " [0.6613863  0.3386137 ]\n",
      " [0.6853535  0.31464648]\n",
      " [0.7105426  0.28945738]\n",
      " [0.6372781  0.36272192]\n",
      " [0.6608989  0.339101  ]\n",
      " [0.6415336  0.35846642]\n",
      " [0.6526795  0.34732053]\n",
      " [0.67954856 0.32045138]\n",
      " [0.6407972  0.35920277]\n",
      " [0.6583191  0.34168085]\n",
      " [0.67739624 0.32260376]\n",
      " [0.62950635 0.37049365]\n",
      " [0.6790321  0.32096797]\n",
      " [0.64391094 0.35608906]\n",
      " [0.6737863  0.32621372]\n",
      " [0.6507468  0.34925318]\n",
      " [0.6615681  0.33843195]\n",
      " [0.64787674 0.35212332]\n",
      " [0.67744315 0.3225568 ]\n",
      " [0.7041579  0.2958421 ]\n",
      " [0.64846647 0.35153356]\n",
      " [0.6355795  0.36442047]\n",
      " [0.6292411  0.37075895]\n",
      " [0.62248564 0.3775144 ]\n",
      " [0.66183543 0.33816457]\n",
      " [0.62740713 0.37259284]\n",
      " [0.68195873 0.31804124]\n",
      " [0.66339695 0.33660302]\n",
      " [0.6816495  0.31835052]\n",
      " [0.70586425 0.29413578]\n",
      " [0.63541734 0.36458272]\n",
      " [0.703393   0.296607  ]\n",
      " [0.69421995 0.30578008]\n",
      " [0.64258116 0.35741884]\n",
      " [0.6291384  0.37086156]\n",
      " [0.62458366 0.3754164 ]\n",
      " [0.6617936  0.3382064 ]\n",
      " [0.66126984 0.33873016]\n",
      " [0.66610605 0.33389398]\n",
      " [0.61379194 0.38620806]\n",
      " [0.6750348  0.32496518]\n",
      " [0.68526244 0.31473756]\n",
      " [0.6082041  0.3917959 ]\n",
      " [0.6110984  0.38890156]\n",
      " [0.6929067  0.3070933 ]\n",
      " [0.6949345  0.30506548]\n",
      " [0.6898933  0.3101067 ]\n",
      " [0.63426876 0.3657312 ]\n",
      " [0.63248706 0.36751297]\n",
      " [0.66984445 0.33015555]\n",
      " [0.65003717 0.34996283]\n",
      " [0.65081906 0.34918094]\n",
      " [0.64862347 0.35137653]\n",
      " [0.6738534  0.3261466 ]\n",
      " [0.6472425  0.35275754]\n",
      " [0.65289885 0.34710118]\n",
      " [0.6688856  0.33111444]\n",
      " [0.6672936  0.3327064 ]\n",
      " [0.62762904 0.37237096]\n",
      " [0.6936041  0.30639595]\n",
      " [0.6616     0.33840004]\n",
      " [0.70426095 0.29573905]\n",
      " [0.6215227  0.37847725]\n",
      " [0.6639823  0.33601773]\n",
      " [0.6135086  0.3864915 ]\n",
      " [0.649557   0.350443  ]\n",
      " [0.61301434 0.38698566]\n",
      " [0.67748517 0.3225148 ]\n",
      " [0.62506014 0.3749399 ]\n",
      " [0.67264485 0.3273551 ]\n",
      " [0.6669216  0.33307838]\n",
      " [0.68573064 0.3142694 ]\n",
      " [0.65266234 0.34733757]\n",
      " [0.6160769  0.38392305]\n",
      " [0.654609   0.34539095]\n",
      " [0.65985936 0.34014064]\n",
      " [0.67947173 0.32052827]\n",
      " [0.6468718  0.3531281 ]\n",
      " [0.6252744  0.37472555]\n",
      " [0.6126116  0.38738844]\n",
      " [0.6541989  0.3458011 ]\n",
      " [0.6618943  0.33810565]\n",
      " [0.6832685  0.31673154]\n",
      " [0.6727575  0.32724246]\n",
      " [0.665337   0.33466294]\n",
      " [0.65391344 0.34608656]\n",
      " [0.66827345 0.33172655]\n",
      " [0.65477633 0.34522364]\n",
      " [0.68083656 0.3191634 ]\n",
      " [0.60439235 0.39560765]\n",
      " [0.6725502  0.32744977]\n",
      " [0.6431234  0.35687667]\n",
      " [0.6430862  0.35691378]\n",
      " [0.6447405  0.35525948]\n",
      " [0.6541707  0.3458293 ]\n",
      " [0.65484655 0.34515348]\n",
      " [0.6653892  0.3346108 ]\n",
      " [0.6101819  0.38981804]\n",
      " [0.66203916 0.33796087]\n",
      " [0.6383479  0.36165208]\n",
      " [0.68786335 0.3121366 ]\n",
      " [0.6554856  0.34451443]\n",
      " [0.6297369  0.37026316]\n",
      " [0.69037324 0.30962676]\n",
      " [0.68322146 0.3167786 ]\n",
      " [0.6540835  0.34591648]\n",
      " [0.6581173  0.34188265]\n",
      " [0.6318442  0.36815572]\n",
      " [0.63556725 0.36443278]\n",
      " [0.6517469  0.34825307]\n",
      " [0.6488073  0.35119268]\n",
      " [0.6495325  0.3504675 ]\n",
      " [0.6874064  0.31259358]\n",
      " [0.59905183 0.4009481 ]\n",
      " [0.64212406 0.3578759 ]\n",
      " [0.65270877 0.34729117]\n",
      " [0.6767756  0.32322448]\n",
      " [0.6305617  0.36943832]\n",
      " [0.6660512  0.3339488 ]\n",
      " [0.6129226  0.38707748]\n",
      " [0.6739805  0.32601953]\n",
      " [0.6592445  0.34075546]\n",
      " [0.66382533 0.33617464]\n",
      " [0.66405386 0.33594614]\n",
      " [0.6150436  0.38495636]\n",
      " [0.65149933 0.34850064]\n",
      " [0.68086356 0.31913647]\n",
      " [0.6644267  0.33557335]\n",
      " [0.6626803  0.33731964]\n",
      " [0.6799185  0.32008144]\n",
      " [0.6612469  0.3387531 ]\n",
      " [0.7080853  0.2919147 ]\n",
      " [0.6663989  0.33360106]\n",
      " [0.6134108  0.38658923]\n",
      " [0.68529236 0.3147077 ]\n",
      " [0.6216706  0.3783294 ]\n",
      " [0.6741763  0.32582372]\n",
      " [0.694989   0.305011  ]\n",
      " [0.6859837  0.31401634]\n",
      " [0.6842189  0.31578115]\n",
      " [0.70297414 0.29702583]\n",
      " [0.62377024 0.37622976]\n",
      " [0.67585284 0.32414725]\n",
      " [0.6442141  0.35578594]\n",
      " [0.64091295 0.35908714]\n",
      " [0.65693367 0.34306636]\n",
      " [0.6463226  0.3536774 ]\n",
      " [0.64456284 0.35543713]\n",
      " [0.6462346  0.3537654 ]\n",
      " [0.67365247 0.32634753]\n",
      " [0.65534794 0.34465206]\n",
      " [0.6798473  0.32015267]\n",
      " [0.6339863  0.3660137 ]\n",
      " [0.6355797  0.36442026]\n",
      " [0.6447094  0.35529056]\n",
      " [0.66861457 0.33138543]\n",
      " [0.69175655 0.3082434 ]\n",
      " [0.6922343  0.3077657 ]\n",
      " [0.6374242  0.36257583]\n",
      " [0.67672765 0.32327226]\n",
      " [0.660183   0.33981702]\n",
      " [0.6343957  0.36560422]\n",
      " [0.6525213  0.34747863]\n",
      " [0.6443362  0.35566375]\n",
      " [0.69522494 0.304775  ]\n",
      " [0.618168   0.38183203]\n",
      " [0.6914871  0.30851296]\n",
      " [0.6435003  0.35649964]\n",
      " [0.65017295 0.34982705]\n",
      " [0.64814913 0.35185087]\n",
      " [0.6537907  0.34620932]\n",
      " [0.666242   0.33375794]\n",
      " [0.62508553 0.37491444]\n",
      " [0.66147786 0.3385222 ]\n",
      " [0.6996345  0.30036548]\n",
      " [0.6214821  0.3785179 ]\n",
      " [0.64037573 0.35962427]\n",
      " [0.64619553 0.3538045 ]\n",
      " [0.66790724 0.3320927 ]\n",
      " [0.64466417 0.35533583]\n",
      " [0.68508595 0.31491405]\n",
      " [0.65526825 0.3447317 ]\n",
      " [0.593496   0.40650403]\n",
      " [0.6544929  0.34550714]\n",
      " [0.6611891  0.33881083]\n",
      " [0.6385324  0.3614676 ]\n",
      " [0.6493415  0.35065854]\n",
      " [0.6256161  0.3743839 ]\n",
      " [0.65575373 0.34424618]\n",
      " [0.6743416  0.32565838]\n",
      " [0.6311374  0.3688626 ]\n",
      " [0.62984765 0.37015235]\n",
      " [0.6521358  0.34786418]\n",
      " [0.63833153 0.36166853]\n",
      " [0.66706467 0.33293536]\n",
      " [0.69224554 0.3077545 ]\n",
      " [0.6113564  0.3886436 ]\n",
      " [0.66527945 0.33472064]\n",
      " [0.6324204  0.36757955]\n",
      " [0.66578865 0.33421135]\n",
      " [0.66624486 0.33375517]\n",
      " [0.6852413  0.31475875]\n",
      " [0.62398654 0.3760135 ]\n",
      " [0.6579823  0.3420177 ]\n",
      " [0.6702265  0.32977352]\n",
      " [0.6705256  0.32947442]\n",
      " [0.6496193  0.35038075]\n",
      " [0.6421097  0.35789028]\n",
      " [0.6474811  0.35251895]\n",
      " [0.6821009  0.3178991 ]\n",
      " [0.65334624 0.34665367]\n",
      " [0.63049376 0.36950624]\n",
      " [0.5949888  0.40501124]\n",
      " [0.63104916 0.36895093]\n",
      " [0.63209474 0.36790526]\n",
      " [0.6419369  0.3580631 ]\n",
      " [0.6676604  0.33233956]\n",
      " [0.6627364  0.33726358]\n",
      " [0.6645223  0.3354777 ]\n",
      " [0.66452736 0.33547264]\n",
      " [0.672788   0.32721198]\n",
      " [0.6340639  0.36593607]\n",
      " [0.64698416 0.35301587]\n",
      " [0.6333434  0.3666566 ]\n",
      " [0.65730715 0.3426928 ]\n",
      " [0.69983345 0.30016658]\n",
      " [0.66961354 0.33038646]\n",
      " [0.64827055 0.35172945]\n",
      " [0.6349928  0.36500713]\n",
      " [0.6704593  0.32954076]\n",
      " [0.69431645 0.30568352]\n",
      " [0.7056702  0.29432982]\n",
      " [0.6583347  0.34166533]\n",
      " [0.63438433 0.36561567]\n",
      " [0.686415   0.31358495]\n",
      " [0.6703708  0.3296292 ]\n",
      " [0.64055645 0.3594436 ]\n",
      " [0.6442094  0.35579062]\n",
      " [0.64819896 0.35180104]\n",
      " [0.6289688  0.3710312 ]\n",
      " [0.7037958  0.29620424]\n",
      " [0.60721904 0.39278096]\n",
      " [0.63271654 0.3672835 ]\n",
      " [0.6227612  0.37723878]\n",
      " [0.6612184  0.33878157]\n",
      " [0.644929   0.35507098]\n",
      " [0.6659599  0.3340401 ]\n",
      " [0.6579238  0.34207618]\n",
      " [0.6918008  0.30819923]\n",
      " [0.65076214 0.34923786]\n",
      " [0.68330264 0.31669733]\n",
      " [0.6242186  0.37578136]\n",
      " [0.6645618  0.33543822]\n",
      " [0.64266163 0.35733834]\n",
      " [0.66660637 0.3333936 ]\n",
      " [0.68762004 0.3123799 ]\n",
      " [0.67587566 0.3241243 ]\n",
      " [0.6720791  0.32792097]\n",
      " [0.6675466  0.3324534 ]\n",
      " [0.6745451  0.32545492]\n",
      " [0.63728064 0.36271933]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_dict['seqs'])\n",
    "print(preds)\n",
    "\n",
    "decision_thres = 0.5 # for classification\n",
    "if regression:\n",
    "    decision_thres = 0 # 0 is the natural border between tolerant and intolerant gwRVIS values\n",
    "\n",
    "preds[preds >= decision_thres] = 1\n",
    "preds[preds < decision_thres] = 0\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "preds_flat = np.argmax(preds, axis=1)\n",
    "test_flat = np.argmax(test_dict[y], axis=1)\n",
    "\n",
    "print(preds_flat)\n",
    "# print(accuracy_score(test_flat, preds_flat))\n",
    "# print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "# roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "# print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160,)\n",
      "(150,)\n",
      "(286,)\n",
      "(24,)\n",
      "Accuracy: 0.5806451612903226\n",
      "[[158   2]\n",
      " [128  22]]\n",
      "ROC AUC: 0.5670833333333334\n"
     ]
    }
   ],
   "source": [
    "print(test_flat[ test_flat == 0 ].shape)\n",
    "print(test_flat[ test_flat == 1 ].shape)\n",
    "\n",
    "print(preds_flat[ preds_flat == 0 ].shape)\n",
    "print(preds_flat[ preds_flat == 1 ].shape)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_flat, preds_flat))\n",
    "print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djifos/.conda/envs/tf/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.   , 0.775, 1.   ]), array([0.        , 0.89333333, 1.        ]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_roc_curve(test_flat, preds_flat, make_plot=True):\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_flat, preds_flat)\n",
    "    roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "\n",
    "    if make_plot:\n",
    "        f = plt.figure(figsize=(6, 6))\n",
    "        _ = plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "        _ = plt.plot([0, 1], [0, 1], '--', linewidth=0.5)  # random predictions curve\n",
    "\n",
    "        _ = plt.xlim([0.0, 1.0])\n",
    "        _ = plt.ylim([0.0, 1.0])\n",
    "        _ = plt.title('\\nROC (area = %0.3f)' % roc_auc)\n",
    "        _ = plt.xlabel('False Positive Rate (1 â€” Specificity)')\n",
    "        _ = plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        f.savefig(\"ROC_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "    return fpr, tpr\n",
    "    \n",
    "plot_roc_curve(test_flat, preds_flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

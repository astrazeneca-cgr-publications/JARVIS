{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import nn_models\n",
    "\n",
    "import sys, os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..')) \n",
    "import custom_utils\n",
    "\n",
    "\n",
    "def init_dirs(config_file):\n",
    "    out_dir = custom_utils.create_out_dir(config_file)\n",
    "    out_dir = '../' + out_dir\t\n",
    "    ml_data_dir = out_dir + '/ml_data'\n",
    "\n",
    "    return out_dir, ml_data_dir\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    print('> Loading data (top ' + str(100 * float(top_ratio)) + ' %) ...')\n",
    "    top_ratio_str = '.top_' + str(top_ratio)\n",
    "\n",
    "    print('Reading training data...')\n",
    "    pkl_in = open(ml_data_dir + '/train' + top_ratio_str + '.pkl', 'rb')\n",
    "    train_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    print('Reading validation data...')\n",
    "    pkl_in = open(ml_data_dir + '/validation' + top_ratio_str + '.pkl', 'rb')\n",
    "    validation_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    print('Reading test data...')\n",
    "    pkl_in = open(ml_data_dir + '/test' + top_ratio_str + '.pkl', 'rb')\n",
    "    test_dict = pickle.load(pkl_in)\n",
    "    pkl_in.close()\n",
    "\n",
    "    return train_dict, validation_dict, test_dict\n",
    "\n",
    "\n",
    "def inspect_input_data(train_dict, validation_dict, test_dict):\n",
    "\n",
    "    print('\\n> Train:')\n",
    "    print(train_dict['X'].shape)\n",
    "    print(train_dict[y].shape)\n",
    "    print(train_dict['seqs'].shape)\n",
    "\n",
    "    print('\\n> Validation:')\n",
    "    print(validation_dict['X'].shape)\n",
    "    print(validation_dict[y].shape)\n",
    "    print(validation_dict['seqs'].shape)\n",
    "\n",
    "    print('\\n> Test:')\n",
    "    print(test_dict['X'].shape)\n",
    "    print(test_dict[y].shape)\n",
    "    print(test_dict['seqs'].shape)\n",
    "\n",
    "\n",
    "\n",
    "def compile_and_train_model(model, epochs=40):\n",
    "\n",
    "    if regression:\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['cosine'])\n",
    "\n",
    "    checkpoint_name = 'gwrvis_best_model.hdf5'\n",
    "    checkpointer = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    model.fit(train_dict['seqs'], train_dict[y], batch_size=2048, epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_data=(validation_dict['seqs'], validation_dict[y]), \n",
    "          callbacks=[checkpointer,earlystopper])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_and_evaluate_model(model):\n",
    "    test_results = model.evaluate(test_dict['seqs'], test_dict[y]) \n",
    "    print(test_results)\n",
    "\n",
    "    preds = model.predict(test_dict['seqs'])\n",
    "    decision_thres = 0.5 # for classification\n",
    "    if regression:\n",
    "        decision_thres = 0 # 0 is the natural border between tolerant and intolerant gwRVIS values\n",
    "\n",
    "    preds[preds >= decision_thres] = 1\n",
    "    preds[preds < decision_thres] = 0\n",
    "\n",
    "    preds_flat = preds.flatten()\n",
    "    test_flat = test_dict[y].flatten()\n",
    "\n",
    "    print(accuracy_score(test_flat, preds_flat))\n",
    "    print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "    roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "    print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading data (top 0.1 %) ...\n",
      "Reading training data...\n",
      "Reading validation data...\n",
      "Reading test data...\n",
      "\n",
      "> Train:\n",
      "(1240, 5)\n",
      "(1240, 2)\n",
      "(1240, 3000, 4)\n",
      "\n",
      "> Validation:\n",
      "(64, 5)\n",
      "(64, 2)\n",
      "(64, 3000, 4)\n",
      "\n",
      "> Test:\n",
      "(310, 5)\n",
      "(310, 2)\n",
      "(310, 3000, 4)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 2971, 16)          1936      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 198, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 198, 16)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 198, 640)          862720    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 198, 640)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 126720)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16220288  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 17,085,202\n",
      "Trainable params: 17,085,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1240 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "1240/1240 [==============================] - 60s 48ms/step - loss: 0.7024 - cosine_proximity: -0.7007 - val_loss: 7.5422 - val_cosine_proximity: -0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.54219, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 2/10\n",
      "1240/1240 [==============================] - 65s 53ms/step - loss: 7.5054 - cosine_proximity: -0.5040 - val_loss: 4.5274 - val_cosine_proximity: -0.5001\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.54219 to 4.52738, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 3/10\n",
      "1240/1240 [==============================] - 58s 47ms/step - loss: 4.4811 - cosine_proximity: -0.4961 - val_loss: 2.3866 - val_cosine_proximity: -0.5045\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.52738 to 2.38658, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 4/10\n",
      "1240/1240 [==============================] - 55s 44ms/step - loss: 2.3707 - cosine_proximity: -0.5008 - val_loss: 0.7018 - val_cosine_proximity: -0.7011\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.38658 to 0.70182, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 5/10\n",
      "1240/1240 [==============================] - 55s 44ms/step - loss: 0.7017 - cosine_proximity: -0.7012 - val_loss: 0.8450 - val_cosine_proximity: -0.6304\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.70182\n",
      "Epoch 6/10\n",
      "1240/1240 [==============================] - 61s 49ms/step - loss: 0.8376 - cosine_proximity: -0.6343 - val_loss: 0.6505 - val_cosine_proximity: -0.7323\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.70182 to 0.65053, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 7/10\n",
      "1240/1240 [==============================] - 56s 45ms/step - loss: 0.6507 - cosine_proximity: -0.7329 - val_loss: 0.8315 - val_cosine_proximity: -0.6848\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.65053\n",
      "Epoch 8/10\n",
      "1240/1240 [==============================] - 58s 47ms/step - loss: 0.7850 - cosine_proximity: -0.6909 - val_loss: 0.6668 - val_cosine_proximity: -0.7367\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.65053\n",
      "Epoch 9/10\n",
      "1240/1240 [==============================] - 55s 44ms/step - loss: 0.6505 - cosine_proximity: -0.7398 - val_loss: 0.6338 - val_cosine_proximity: -0.7435\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.65053 to 0.63377, saving model to gwrvis_best_model.hdf5\n",
      "Epoch 10/10\n",
      "1240/1240 [==============================] - 56s 45ms/step - loss: 0.6365 - cosine_proximity: -0.7413 - val_loss: 0.6762 - val_cosine_proximity: -0.7146\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63377\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    config_file = '../config.yaml' #sys.argv[1]\n",
    "    top_ratio = 0.001 #sys.argv[2] \t#default: 0.01 -- look at top 1% of intolerant/tolerant windows\n",
    "\n",
    "    regression=False\n",
    "    if regression:\n",
    "        y = 'gwrvis'   # continuous value\n",
    "    else:\n",
    "        y = 'y'  # 1/0 annotation\n",
    "\n",
    "    config_params = custom_utils.get_config_params(config_file)\n",
    "    win_len = config_params['win_len']\n",
    "\n",
    "    # init out dir structure\n",
    "    out_dir, ml_data_dir = init_dirs(config_file)\n",
    "\n",
    "    # read train, validation, test data\n",
    "    train_dict, validation_dict, test_dict = read_data()\n",
    "\n",
    "    # print input data shapes to check for consistency\n",
    "    inspect_input_data(train_dict, validation_dict, test_dict)\n",
    "\n",
    "    #model = nn_models.cnn_1_conv_2_fcc(regression=regression) \n",
    "    model = nn_models.cnn_rnn_1_conv_1_lstm(regression=regression)\n",
    "    print(model.summary())\n",
    "\n",
    "    model = compile_and_train_model(model, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['gwrvis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38635784 0.61364216]\n",
      " [0.23948577 0.7605143 ]\n",
      " [0.24778436 0.7522156 ]\n",
      " [0.5161654  0.48383468]\n",
      " [0.6138244  0.38617554]\n",
      " [0.5252398  0.47476017]\n",
      " [0.57467455 0.42532548]\n",
      " [0.56012976 0.43987024]\n",
      " [0.23780088 0.7621991 ]\n",
      " [0.21731295 0.78268707]\n",
      " [0.4694343  0.53056574]\n",
      " [0.19750874 0.80249125]\n",
      " [0.35222748 0.64777243]\n",
      " [0.6007285  0.3992715 ]\n",
      " [0.27218783 0.7278122 ]\n",
      " [0.19937176 0.80062824]\n",
      " [0.22824971 0.77175033]\n",
      " [0.47077364 0.5292264 ]\n",
      " [0.4463056  0.55369437]\n",
      " [0.47552165 0.5244784 ]\n",
      " [0.26611894 0.73388106]\n",
      " [0.4261733  0.57382673]\n",
      " [0.29003727 0.7099627 ]\n",
      " [0.543739   0.45626098]\n",
      " [0.2590626  0.7409375 ]\n",
      " [0.49579215 0.5042078 ]\n",
      " [0.20262912 0.7973709 ]\n",
      " [0.50714535 0.49285462]\n",
      " [0.27021974 0.72978026]\n",
      " [0.19340652 0.8065934 ]\n",
      " [0.263356   0.736644  ]\n",
      " [0.21823543 0.7817646 ]\n",
      " [0.18940392 0.8105961 ]\n",
      " [0.44658446 0.5534156 ]\n",
      " [0.199038   0.80096203]\n",
      " [0.46457148 0.5354285 ]\n",
      " [0.28001127 0.71998876]\n",
      " [0.39928678 0.6007132 ]\n",
      " [0.2638685  0.7361314 ]\n",
      " [0.24672599 0.75327396]\n",
      " [0.34244674 0.65755326]\n",
      " [0.51393414 0.4860659 ]\n",
      " [0.5949503  0.40504968]\n",
      " [0.20133549 0.7986645 ]\n",
      " [0.23914482 0.7608552 ]\n",
      " [0.39795634 0.6020437 ]\n",
      " [0.42039752 0.57960254]\n",
      " [0.32692763 0.6730724 ]\n",
      " [0.517521   0.48247895]\n",
      " [0.5272563  0.47274363]\n",
      " [0.31662253 0.6833775 ]\n",
      " [0.52852106 0.4714789 ]\n",
      " [0.26471078 0.7352892 ]\n",
      " [0.29013136 0.70986867]\n",
      " [0.2427532  0.75724685]\n",
      " [0.29318893 0.7068111 ]\n",
      " [0.42225695 0.577743  ]\n",
      " [0.3121418  0.68785816]\n",
      " [0.27420682 0.7257932 ]\n",
      " [0.29636136 0.7036386 ]\n",
      " [0.18640345 0.8135966 ]\n",
      " [0.46184817 0.53815186]\n",
      " [0.44795108 0.55204886]\n",
      " [0.31483045 0.6851695 ]\n",
      " [0.19011897 0.809881  ]\n",
      " [0.28328624 0.7167137 ]\n",
      " [0.33941272 0.6605873 ]\n",
      " [0.36289898 0.63710105]\n",
      " [0.5347789  0.46522114]\n",
      " [0.27840695 0.72159296]\n",
      " [0.225913   0.7740871 ]\n",
      " [0.20762481 0.79237515]\n",
      " [0.27839252 0.72160745]\n",
      " [0.52956533 0.47043464]\n",
      " [0.22278415 0.77721584]\n",
      " [0.30561197 0.6943881 ]\n",
      " [0.21213993 0.7878601 ]\n",
      " [0.49865085 0.50134915]\n",
      " [0.52548236 0.47451767]\n",
      " [0.19450511 0.80549484]\n",
      " [0.5344266  0.46557343]\n",
      " [0.56080174 0.43919823]\n",
      " [0.19608943 0.80391055]\n",
      " [0.23076765 0.76923233]\n",
      " [0.19215088 0.8078491 ]\n",
      " [0.29724765 0.7027523 ]\n",
      " [0.2810297  0.7189703 ]\n",
      " [0.47596908 0.5240309 ]\n",
      " [0.19230449 0.80769557]\n",
      " [0.31836465 0.6816353 ]\n",
      " [0.55466115 0.44533882]\n",
      " [0.18890293 0.8110971 ]\n",
      " [0.23494533 0.7650547 ]\n",
      " [0.5827327  0.41726735]\n",
      " [0.4447743  0.55522573]\n",
      " [0.28693378 0.7130662 ]\n",
      " [0.19015127 0.8098487 ]\n",
      " [0.20441113 0.79558885]\n",
      " [0.4507573  0.5492427 ]\n",
      " [0.25980932 0.7401907 ]\n",
      " [0.25638288 0.7436171 ]\n",
      " [0.36321175 0.63678825]\n",
      " [0.35263997 0.64736   ]\n",
      " [0.25272098 0.747279  ]\n",
      " [0.22478087 0.7752192 ]\n",
      " [0.24329165 0.7567083 ]\n",
      " [0.44284275 0.5571573 ]\n",
      " [0.20252186 0.79747814]\n",
      " [0.5675985  0.43240142]\n",
      " [0.5108324  0.48916757]\n",
      " [0.42874008 0.5712599 ]\n",
      " [0.18733577 0.8126643 ]\n",
      " [0.2294592  0.7705408 ]\n",
      " [0.18904993 0.8109501 ]\n",
      " [0.37077388 0.62922615]\n",
      " [0.20688626 0.79311377]\n",
      " [0.37403765 0.6259624 ]\n",
      " [0.19039936 0.8096006 ]\n",
      " [0.49728677 0.50271326]\n",
      " [0.24777539 0.7522246 ]\n",
      " [0.5406965  0.45930353]\n",
      " [0.22163329 0.77836674]\n",
      " [0.23362158 0.7663784 ]\n",
      " [0.18986869 0.8101314 ]\n",
      " [0.44439474 0.55560523]\n",
      " [0.43726286 0.56273717]\n",
      " [0.19889057 0.8011094 ]\n",
      " [0.36682045 0.63317955]\n",
      " [0.19792108 0.8020789 ]\n",
      " [0.18885562 0.8111444 ]\n",
      " [0.41853535 0.5814646 ]\n",
      " [0.3097293  0.6902707 ]\n",
      " [0.26215708 0.73784286]\n",
      " [0.35598826 0.64401174]\n",
      " [0.25713393 0.74286604]\n",
      " [0.2994575  0.70054245]\n",
      " [0.2382589  0.76174116]\n",
      " [0.44097963 0.55902034]\n",
      " [0.33207086 0.6679292 ]\n",
      " [0.46066913 0.5393309 ]\n",
      " [0.24940787 0.75059205]\n",
      " [0.319085   0.680915  ]\n",
      " [0.43948087 0.5605191 ]\n",
      " [0.2263696  0.7736304 ]\n",
      " [0.20758228 0.7924177 ]\n",
      " [0.31692785 0.68307215]\n",
      " [0.27521485 0.72478515]\n",
      " [0.50269    0.49730998]\n",
      " [0.19196598 0.808034  ]\n",
      " [0.5867108  0.41328916]\n",
      " [0.20263258 0.7973674 ]\n",
      " [0.24482045 0.7551795 ]\n",
      " [0.56521    0.43479   ]\n",
      " [0.41246846 0.58753157]\n",
      " [0.2539884  0.7460116 ]\n",
      " [0.25373942 0.7462606 ]\n",
      " [0.18289872 0.81710124]\n",
      " [0.31241083 0.6875891 ]\n",
      " [0.3522843  0.6477156 ]\n",
      " [0.33030128 0.6696987 ]\n",
      " [0.20252974 0.7974703 ]\n",
      " [0.5535247  0.44647536]\n",
      " [0.19209416 0.8079059 ]\n",
      " [0.2659381  0.7340619 ]\n",
      " [0.3093807  0.69061923]\n",
      " [0.42444223 0.57555777]\n",
      " [0.18637174 0.81362826]\n",
      " [0.30587092 0.69412905]\n",
      " [0.18967894 0.81032103]\n",
      " [0.23029725 0.76970273]\n",
      " [0.2897905  0.7102095 ]\n",
      " [0.5153283  0.48467174]\n",
      " [0.37297106 0.62702894]\n",
      " [0.20364031 0.7963597 ]\n",
      " [0.30732602 0.692674  ]\n",
      " [0.52666825 0.47333175]\n",
      " [0.32517982 0.6748202 ]\n",
      " [0.5068624  0.49313763]\n",
      " [0.3368374  0.66316265]\n",
      " [0.49700508 0.50299495]\n",
      " [0.547168   0.45283195]\n",
      " [0.30767313 0.69232684]\n",
      " [0.18372858 0.81627136]\n",
      " [0.41788542 0.5821146 ]\n",
      " [0.20993596 0.7900641 ]\n",
      " [0.56243545 0.43756458]\n",
      " [0.5891028  0.41089723]\n",
      " [0.50834537 0.4916547 ]\n",
      " [0.4916232  0.5083768 ]\n",
      " [0.47747692 0.5225231 ]\n",
      " [0.1862499  0.8137501 ]\n",
      " [0.40272173 0.5972783 ]\n",
      " [0.21769723 0.7823028 ]\n",
      " [0.21014732 0.7898527 ]\n",
      " [0.28012425 0.7198757 ]\n",
      " [0.19356395 0.806436  ]\n",
      " [0.25661832 0.7433817 ]\n",
      " [0.26734328 0.7326567 ]\n",
      " [0.54238117 0.45761877]\n",
      " [0.32465822 0.6753418 ]\n",
      " [0.42983374 0.5701663 ]\n",
      " [0.33848462 0.6615154 ]\n",
      " [0.25692648 0.7430736 ]\n",
      " [0.54955184 0.45044813]\n",
      " [0.47057804 0.529422  ]\n",
      " [0.2622502  0.7377498 ]\n",
      " [0.59315413 0.4068458 ]\n",
      " [0.27478737 0.72521263]\n",
      " [0.37245142 0.6275486 ]\n",
      " [0.30872476 0.6912752 ]\n",
      " [0.19004261 0.8099574 ]\n",
      " [0.18970472 0.8102953 ]\n",
      " [0.25091264 0.74908733]\n",
      " [0.5248913  0.4751087 ]\n",
      " [0.19245023 0.8075498 ]\n",
      " [0.46554953 0.5344504 ]\n",
      " [0.19222774 0.8077722 ]\n",
      " [0.40865362 0.5913464 ]\n",
      " [0.32218885 0.67781115]\n",
      " [0.18196827 0.8180317 ]\n",
      " [0.53578293 0.4642171 ]\n",
      " [0.19106059 0.8089394 ]\n",
      " [0.28197828 0.71802175]\n",
      " [0.5057705  0.4942295 ]\n",
      " [0.18850149 0.8114985 ]\n",
      " [0.31828472 0.68171525]\n",
      " [0.24371526 0.7562847 ]\n",
      " [0.5552976  0.44470245]\n",
      " [0.2297337  0.7702663 ]\n",
      " [0.5081492  0.4918508 ]\n",
      " [0.2926766  0.70732343]\n",
      " [0.19453995 0.80546004]\n",
      " [0.22482312 0.77517694]\n",
      " [0.54239726 0.45760268]\n",
      " [0.20866239 0.79133755]\n",
      " [0.21060173 0.7893982 ]\n",
      " [0.23393533 0.76606464]\n",
      " [0.4259193  0.57408065]\n",
      " [0.42447633 0.5755237 ]\n",
      " [0.18865895 0.8113411 ]\n",
      " [0.1915919  0.8084081 ]\n",
      " [0.21160504 0.788395  ]\n",
      " [0.2496535  0.7503465 ]\n",
      " [0.4132917  0.58670837]\n",
      " [0.3824502  0.61754984]\n",
      " [0.20132671 0.79867333]\n",
      " [0.36212552 0.63787454]\n",
      " [0.23294877 0.7670512 ]\n",
      " [0.44668475 0.55331516]\n",
      " [0.39122415 0.6087759 ]\n",
      " [0.41037905 0.58962095]\n",
      " [0.330878   0.669122  ]\n",
      " [0.25459167 0.7454083 ]\n",
      " [0.46165848 0.53834146]\n",
      " [0.50321203 0.49678797]\n",
      " [0.24727933 0.75272065]\n",
      " [0.36778498 0.632215  ]\n",
      " [0.22627532 0.77372473]\n",
      " [0.44881862 0.5511814 ]\n",
      " [0.22131966 0.7786803 ]\n",
      " [0.1963114  0.8036886 ]\n",
      " [0.18724568 0.81275433]\n",
      " [0.22263435 0.7773657 ]\n",
      " [0.21783772 0.7821623 ]\n",
      " [0.395019   0.60498095]\n",
      " [0.4430601  0.55693984]\n",
      " [0.3842947  0.6157053 ]\n",
      " [0.36672282 0.6332771 ]\n",
      " [0.29584995 0.7041501 ]\n",
      " [0.4073183  0.5926817 ]\n",
      " [0.21734425 0.7826558 ]\n",
      " [0.21924013 0.78075993]\n",
      " [0.24225368 0.75774634]\n",
      " [0.18957554 0.81042445]\n",
      " [0.35292402 0.64707595]\n",
      " [0.32376322 0.6762368 ]\n",
      " [0.21812496 0.7818751 ]\n",
      " [0.5539356  0.44606444]\n",
      " [0.22722815 0.77277184]\n",
      " [0.50264144 0.4973586 ]\n",
      " [0.57630974 0.4236902 ]\n",
      " [0.35654312 0.6434569 ]\n",
      " [0.19005783 0.80994225]\n",
      " [0.2698654  0.7301346 ]\n",
      " [0.46306878 0.5369313 ]\n",
      " [0.31602782 0.6839722 ]\n",
      " [0.3903833  0.6096167 ]\n",
      " [0.30885345 0.69114655]\n",
      " [0.2016285  0.7983715 ]\n",
      " [0.5204368  0.47956315]\n",
      " [0.19685458 0.8031454 ]\n",
      " [0.20913783 0.7908621 ]\n",
      " [0.19359292 0.8064071 ]\n",
      " [0.5478817  0.4521183 ]\n",
      " [0.20195891 0.7980411 ]\n",
      " [0.49545166 0.5045484 ]\n",
      " [0.22013    0.77987003]\n",
      " [0.6137281  0.38627183]\n",
      " [0.27557284 0.7244271 ]\n",
      " [0.50702894 0.49297106]\n",
      " [0.20638283 0.7936172 ]\n",
      " [0.42963886 0.57036114]\n",
      " [0.35863602 0.6413639 ]\n",
      " [0.21233194 0.78766805]\n",
      " [0.4140217  0.58597827]\n",
      " [0.38708904 0.6129109 ]\n",
      " [0.51618165 0.4838184 ]\n",
      " [0.48307842 0.5169215 ]\n",
      " [0.48762652 0.5123735 ]\n",
      " [0.20924702 0.79075295]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_dict['seqs'])\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "decision_thres = 0.5 # for classification\n",
    "if regression:\n",
    "    decision_thres = 0 # 0 is the natural border between tolerant and intolerant gwRVIS values\n",
    "\n",
    "preds[preds >= decision_thres] = 1\n",
    "preds[preds < decision_thres] = 0\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "preds_flat = np.argmax(preds, axis=1)\n",
    "test_flat = np.argmax(test_dict[y], axis=1)\n",
    "\n",
    "print(preds_flat)\n",
    "# print(accuracy_score(test_flat, preds_flat))\n",
    "# print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "# roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "# print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160,)\n",
      "(150,)\n",
      "(52,)\n",
      "(258,)\n"
     ]
    }
   ],
   "source": [
    "print(test_flat[ test_flat == 0 ].shape)\n",
    "print(test_flat[ test_flat == 1 ].shape)\n",
    "\n",
    "print(preds_flat[ preds_flat == 0 ].shape)\n",
    "print(preds_flat[ preds_flat == 1 ].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5483870967741935\n",
      "[[ 36 124]\n",
      " [ 16 134]]\n",
      "ROC AUC: 0.5591666666666666\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_flat, preds_flat))\n",
    "print(confusion_matrix(test_flat, preds_flat))\n",
    "\n",
    "roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "print('ROC AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djifos/.conda/envs/tf/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.   , 0.775, 1.   ]), array([0.        , 0.89333333, 1.        ]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_roc_curve(test_flat, preds_flat, make_plot=True):\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_flat, preds_flat)\n",
    "    roc_auc = roc_auc_score(test_flat, preds_flat)\n",
    "\n",
    "    if make_plot:\n",
    "        f = plt.figure(figsize=(6, 6))\n",
    "        _ = plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "        _ = plt.plot([0, 1], [0, 1], '--', linewidth=0.5)  # random predictions curve\n",
    "\n",
    "        _ = plt.xlim([0.0, 1.0])\n",
    "        _ = plt.ylim([0.0, 1.0])\n",
    "        _ = plt.title('\\nROC (area = %0.3f)' % roc_auc)\n",
    "        _ = plt.xlabel('False Positive Rate (1 â€” Specificity)')\n",
    "        _ = plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        f.savefig(\"ROC_curve.pdf\", bbox_inches='tight')\n",
    "\n",
    "    return fpr, tpr\n",
    "    \n",
    "plot_roc_curve(test_flat, preds_flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
